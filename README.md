<div align="center">

# ğŸ”¥ Transformer from Scratch

**A complete Transformer implementation built entirely from first principles in PyTorch.**

No `nn.Linear`. No `nn.Embedding`. No `nn.LayerNorm`. Just raw math.

[![Python 3.10+](https://img.shields.io/badge/Python-3.10%2B-3776AB?logo=python&logoColor=white)](https://python.org)
[![PyTorch 2.0+](https://img.shields.io/badge/PyTorch-2.0%2B-EE4C2C?logo=pytorch&logoColor=white)](https://pytorch.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

[Architecture](#architecture) Â· [Quick Start](#quick-start) Â· [Training Results](#training-results) Â· [Project Structure](#project-structure)

</div>

---

## âœ¨ What Makes This Special

This isn't a wrapper around `nn.TransformerEncoder`. Every single component is implemented from scratch using only `torch.Tensor` operations and `nn.Parameter`:

| Component | Standard PyTorch | **This Repo** |
|---|---|---|
| Linear Layer | `nn.Linear` | **Custom `Linear`** â€” bias-free `y = xW^T` |
| Embedding | `nn.Embedding` | **Custom `Embedding`** â€” raw lookup table |
| Normalization | `nn.LayerNorm` | **RMSNorm** â€” faster, no mean subtraction |
| Feed-Forward | `ReLU(xWâ‚)Wâ‚‚` | **SwiGLU** â€” gated activation (LLaMA-style) |
| Positional Encoding | Sinusoidal / Learned | **RoPE** â€” rotary embeddings |
| Training | Single GPU | **DDP + Mixed Precision (FP16)** |

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TRANSFORMER                              â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚      ENCODER (Ã—6)    â”‚    â”‚        DECODER (Ã—6)          â”‚   â”‚
â”‚  â”‚                      â”‚    â”‚                              â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚   RMSNorm      â”‚  â”‚    â”‚  â”‚   RMSNorm              â”‚  â”‚   â”‚
â”‚  â”‚  â”‚   Self-Attn    â”‚  â”‚    â”‚  â”‚   Masked Self-Attn     â”‚  â”‚   â”‚
â”‚  â”‚  â”‚   + RoPE       â”‚â”€â”€â”‚â”€â”€â”€â”€â”‚â”€â”€â”‚   + RoPE               â”‚  â”‚   â”‚
â”‚  â”‚  â”‚   + Dropout    â”‚  â”‚    â”‚  â”‚   + Dropout             â”‚  â”‚   â”‚
â”‚  â”‚  â”‚   + Residual   â”‚  â”‚    â”‚  â”‚   + Residual            â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚   RMSNorm      â”‚  â”‚    â”‚  â”‚   RMSNorm              â”‚  â”‚   â”‚
â”‚  â”‚  â”‚   SwiGLU FFN   â”‚  â”‚    â”‚  â”‚   Cross-Attention      â”‚  â”‚   â”‚
â”‚  â”‚  â”‚   + Dropout    â”‚  â”‚    â”‚  â”‚   + Dropout             â”‚  â”‚   â”‚
â”‚  â”‚  â”‚   + Residual   â”‚  â”‚    â”‚  â”‚   + Residual            â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚                      â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚                      â”‚    â”‚  â”‚   RMSNorm              â”‚  â”‚   â”‚
â”‚  â”‚                      â”‚    â”‚  â”‚   SwiGLU FFN           â”‚  â”‚   â”‚
â”‚  â”‚                      â”‚    â”‚  â”‚   + Dropout             â”‚  â”‚   â”‚
â”‚  â”‚                      â”‚    â”‚  â”‚   + Residual            â”‚  â”‚   â”‚
â”‚  â”‚                      â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                            â”‚                    â”‚
â”‚                                     â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚                                     â”‚  Projection  â”‚            â”‚
â”‚                                     â”‚ (weight-tied)â”‚            â”‚
â”‚                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Mathematical Foundations

<details>
<summary><b>ğŸ”¢ RMSNorm</b> â€” Root Mean Square Normalization</summary>

```
RMS(x) = âˆš(mean(xÂ²) + Îµ)
output = (x / RMS(x)) Â· Î³
```

Unlike LayerNorm, RMSNorm skips the mean subtraction step, making it simpler and ~10% faster while achieving comparable performance.

**Reference:** Zhang & Sennrich, "[Root Mean Square Layer Normalization](https://arxiv.org/abs/1910.07467)" (2019)
</details>

<details>
<summary><b>ğŸ”„ RoPE</b> â€” Rotary Positional Embeddings</summary>

```
For each dimension pair (2i, 2i+1):
    Î¸áµ¢ = 1 / (10000^(2i/d))

    â”Œ        â”   â”Œ              â” â”Œ        â”
    â”‚ x'â‚‚áµ¢   â”‚ = â”‚ cos(mÎ¸áµ¢)  -sin(mÎ¸áµ¢) â”‚ â”‚ xâ‚‚áµ¢   â”‚
    â”‚ x'â‚‚áµ¢â‚Šâ‚ â”‚   â”‚ sin(mÎ¸áµ¢)   cos(mÎ¸áµ¢) â”‚ â”‚ xâ‚‚áµ¢â‚Šâ‚ â”‚
    â””        â”˜   â””              â”˜ â””        â”˜
```

RoPE encodes position by rotating Q/K vectors, making attention scores depend only on relative distances.

**Reference:** Su et al., "[RoFormer](https://arxiv.org/abs/2104.09864)" (2021)
</details>

<details>
<summary><b>âš¡ SwiGLU</b> â€” Gated Feed-Forward Network</summary>

```
gate   = SiLU(x Â· Wâ‚áµ€)        where SiLU(z) = z Â· Ïƒ(z)
value  = x Â· Wâ‚ƒáµ€
output = (gate âŠ™ value) Â· Wâ‚‚áµ€
```

SwiGLU replaces the standard ReLU FFN with a gated mechanism, improving training efficiency. Hidden dim follows LLaMA: `ceil(8/3 Ã— d_model / 64) Ã— 64`.

**Reference:** Shazeer, "[GLU Variants Improve Transformer](https://arxiv.org/abs/2002.05202)" (2020)
</details>

<details>
<summary><b>ğŸ¯ Scaled Dot-Product Attention</b></summary>

```
Attention(Q, K, V) = softmax(Q Â· Káµ€ / âˆšd_k) Â· V
```

Multi-head attention splits Q, K, V into `h` heads, applies attention independently, and concatenates results.

**Reference:** Vaswani et al., "[Attention Is All You Need](https://arxiv.org/abs/1706.03762)" (2017)
</details>

---

## Quick Start

### Installation

```bash
git clone https://github.com/SahilSharma1306/transformer-from-scratch.git
cd transformer-from-scratch
pip install -r requirements.txt
```

### Training

```bash
# Single GPU
python -m transformer.train --config configs/default.yaml

# Multi-GPU (DDP) â€” auto-detects available GPUs
bash scripts/train.sh

# Custom configuration
python -m transformer.train --config configs/default.yaml --num_epochs 50 --batch_size 64
```

### Translation (after training)

```bash
# Single sentence
python -m transformer.translate --checkpoint weights/tmodel_24.pt --text "Hello world"

# Interactive mode
python -m transformer.translate --checkpoint weights/tmodel_24.pt --interactive
```

### Run Tests

```bash
python -m pytest tests/ -v
```

---

## Training Results

Trained on the **Opus Books** English â†’ Italian dataset (32,332 sentence pairs) using 2Ã— T4 GPUs on Kaggle.

### Configuration

| Hyperparameter | Value |
|---|---|
| Model Dimension (`d_model`) | 512 |
| Attention Heads | 8 |
| Encoder/Decoder Layers | 6 |
| SwiGLU Hidden Dim | 1,408 |
| Vocabulary Size | 20,000 (BPE) |
| Sequence Length | 320 |
| Batch Size | 32 per GPU |
| Peak Learning Rate | 6Ã—10â»â´ |
| Dropout | 0.3 |
| Training | 25 epochs, DDP, FP16 |

### Loss Progression

| Epoch | Train Loss | Val Loss |
|---|---|---|
| 0 | ~7.5 | 6.14 |
| 5 | ~5.3 | 5.02 |
| 10 | ~4.6 | 4.65 |
| 15 | ~3.9 | 4.51 |
| 20 | ~3.7 | 4.47 |
| 24 | ~3.6 | 4.48 |

### Sample Translations (Epoch 24)

| Source (EN) | Target (IT) | Prediction |
|---|---|---|
| "But Oblonsky arranged that too." | "Stepan Arkad'ic accomodÃ² anche questo." | "Ma Stepan Arkad'ic aveva preso questo." |
| "Karenin glanced at him with his weary eyes." | "Aleksej Aleksandrovic lo guardÃ² con occhi stanchi." | "Aleksej Aleksandrovic lo guardÃ² con gli occhi." |
| "'What do you mean?" | "â€” Che cosa allora?" | "â€” Che cosa volete?" |

---

## Project Structure

```
transformer-from-scratch/
â”‚
â”œâ”€â”€ transformer/                 # Core package
â”‚   â”œâ”€â”€ __init__.py              # Package exports
â”‚   â”œâ”€â”€ config.py                # Dataclass config + CLI/YAML parsing
â”‚   â”œâ”€â”€ model.py                 # All model components (from scratch)
â”‚   â”œâ”€â”€ dataset.py               # Data loading + BPE tokenization
â”‚   â”œâ”€â”€ train.py                 # DDP + AMP training loop
â”‚   â”œâ”€â”€ validate.py              # Validation + greedy decoding
â”‚   â””â”€â”€ translate.py             # Standalone inference script
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_model.py            # Unit tests for all components
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml             # Default hyperparameters
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train.sh                 # DDP launch script
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ CONTRIBUTING.md
â”œâ”€â”€ LICENSE                      # MIT
â”œâ”€â”€ Makefile                     # Common operations
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ pyproject.toml               # Python packaging
```

---

## Configuration

All hyperparameters live in a `TransformerConfig` dataclass. Override via:

**YAML file:**
```yaml
# configs/custom.yaml
num_epochs: 50
batch_size: 64
lr: 3e-4
```

**CLI flags** (override YAML):
```bash
python -m transformer.train --config configs/custom.yaml --dropout 0.2
```

**Python:**
```python
from transformer import TransformerConfig
config = TransformerConfig(d_model=768, num_heads=12, num_layers=12)
```

---

## Implementation Details

### Design Decisions

1. **Pre-Norm Residuals** â€” We normalize *before* attention/FFN, not after. This improves training stability and removes the need for learning rate warmup tuning.

2. **Weight Tying** â€” The output projection layer shares its weight matrix with the target embedding, reducing parameters and improving generalization.

3. **No Bias** â€” All `Linear` layers are bias-free, following modern LLM conventions (LLaMA, GPT-NeoX).

4. **DDP-Safe Tokenizer** â€” Only Rank 0 builds the BPE tokenizer; other ranks wait at a `dist.barrier()` before loading from disk.

5. **Truncated Normal Init** â€” Xavier-style initialization with Â±3Ïƒ truncation for stable training from the start.

---

## References

- Vaswani et al., "[Attention Is All You Need](https://arxiv.org/abs/1706.03762)" (2017)
- Su et al., "[RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)" (2021)
- Zhang & Sennrich, "[Root Mean Square Layer Normalization](https://arxiv.org/abs/1910.07467)" (2019)
- Shazeer, "[GLU Variants Improve Transformer](https://arxiv.org/abs/2002.05202)" (2020)
- Touvron et al., "[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)" (2023)

---

## License

This project is licensed under the MIT License â€” see [LICENSE](LICENSE) for details.

---

<div align="center">
<b>Built with â¤ï¸ and raw tensors</b>
</div>
